# üõ°Ô∏è Arquitectura de Sistema Distribuido y Tolerancia a Fallos - QRM

Este documento detalla la arquitectura t√©cnica del proyecto **Quantitative Risk Management (QRM)**, justificando su dise√±o como sistema distribuido, su capacidad de tolerancia a fallos y el modelo operativo implementado.

---

## 1. üåê ¬øPor qu√© un Sistema Distribuido?

El sistema QRM no es una aplicaci√≥n monol√≠tica tradicional, sino un ecosistema de **microservicios independientes** que se comunican entre s√≠. Esta arquitectura fue elegida para garantizar:

*   **Desacoplamiento**: Cada servicio tiene una responsabilidad √∫nica y clara.
*   **Escalabilidad Independiente**: Podemos escalar el servicio de escaneo (que consume mucha CPU) sin afectar al backend o al frontend.
*   **Tecnolog√≠a Agn√≥stica**: Cada microservicio puede usar la tecnolog√≠a m√°s adecuada para su tarea (Python para ML, Node.js para I/O, etc.).

### Componentes del Ecosistema:

1.  **Backend Service (Django)**: Gesti√≥n de usuarios, l√≥gica de negocio central.
2.  **NVD Service (Python)**: Sincronizaci√≥n y consulta de vulnerabilidades (NIST).
3.  **ML Service (Python)**: Modelos de predicci√≥n de riesgo.
4.  **Nmap Scanner Service (Node.js)**: Ejecuci√≥n de escaneos de red (intensivo).
5.  **Frontend (React)**: Interfaz de usuario.
6.  **Health Monitor (Node.js)**: *Nuevo componente* para vigilancia y alertas.

---

## 2. üõ°Ô∏è Tolerancia a Fallos y Resiliencia

La caracter√≠stica m√°s cr√≠tica de este sistema es su capacidad para **seguir operando parcialmente** incluso cuando uno de sus componentes falla.

### Escenario de Fallo: Ca√≠da del Esc√°ner
Si el `nmap-scanner-service` deja de funcionar (por error de memoria, bloqueo, etc.):

1.  **El Frontend sigue accesible**: Los usuarios pueden ver reportes hist√≥ricos y navegar.
2.  **El Backend sigue respondiendo**: La autenticaci√≥n y gesti√≥n de activos funciona.
3.  **NVD y ML siguen operativos**: Se pueden consultar vulnerabilidades y predicciones previas.

**El sistema NO colapsa.** Solo la funcionalidad espec√≠fica de "iniciar nuevos escaneos" se ve afectada temporalmente, pero el negocio contin√∫a.

---

## 3. ‚è∞ Modelo Operativo 8/5 y Monitoreo Activo

El sistema est√° dise√±ado para operar bajo un modelo **8/5 (8 horas diarias, 5 d√≠as a la semana)**, alineado con el horario laboral est√°ndar de la organizaci√≥n.

### ¬øPor qu√© es cr√≠tico el monitoreo en este modelo?
A diferencia de un sistema 24/7 donde suele haber rotaci√≥n de personal, en un modelo 8/5 es vital maximizar la disponibilidad durante las horas productivas.

*   **Detecci√≥n Inmediata**: Si el servicio de escaneo cae a las 10:00 AM, no podemos esperar a que un usuario lo reporte a las 3:00 PM.
*   **Health Monitor**: Hemos implementado un "vigilante" automatizado que chequea la salud de los servicios cr√≠ticos cada **30 segundos**.
*   **Notificaci√≥n Proactiva**: El sistema env√≠a un email a los administradores (`justingomezcoello@gmail.com`) en el momento exacto del fallo.

---

## 4. üè• El Guardi√°n: Health Monitor Service

Es un microservicio ligero dise√±ado con un √∫nico prop√≥sito: **Asegurar la disponibilidad**.

### Funcionamiento T√©cnico:
1.  **Heartbeat Check**: Realiza una petici√≥n HTTP `GET /api/v1/health` al servicio de escaneo cada 30 segundos.
2.  **L√≥gica de Reintentos**: Para evitar falsos positivos, requiere **3 fallos consecutivos** (ventana de 90 segundos) antes de declarar una emergencia.
3.  **Alertas SMTP**: Se conecta a un servidor de correo (actualmente Mailtrap para pruebas, configurable para Outlook/Gmail) para enviar alertas detalladas.
4.  **Auto-Recuperaci√≥n**: Contin√∫a monitoreando y notifica autom√°ticamente cuando el servicio vuelve a estar "Saludable".

---

## 5. üß™ Gu√≠a de Verificaci√≥n (Evidence)

Para demostrar la robustez del sistema, se puede ejecutar el siguiente protocolo de pruebas:

### Paso 1: Verificar Estado Saludable
Con todos los servicios corriendo:
```bash
# El monitor debe reportar estado saludable en los logs
docker-compose logs -f health-monitor
```

### Paso 2: Simular Cat√°strofe (Ca√≠da de Servicio)
Detenemos intencionalmente el servicio cr√≠tico:
```bash
docker-compose stop nmap-scanner-service
```

### Paso 3: Comprobar Tolerancia a Fallos (Independencia)
Mientras el esc√°ner est√° "muerto", verificamos que el resto de la empresa sigue funcionando:

*   **Backend**: `curl http://localhost:8000/api/v1/health` ‚úÖ (Responde 200 OK)
*   **NVD**: `curl http://localhost:8002/api/v1/health` ‚úÖ (Responde 200 OK)
*   **Frontend**: Navegar a `http://localhost:5173` ‚úÖ (Carga correctamente)

### Paso 4: Verificar Alerta Autom√°tica
Esperar 90 segundos y verificar la bandeja de entrada del correo configurado.
*   **Resultado**: Recibo de email con asunto `üö® ALERTA: nmap-scanner-service CA√çDO`.

### Paso 5: Recuperaci√≥n
```bash
docker-compose start nmap-scanner-service
```
*   **Resultado**: El sistema detecta la recuperaci√≥n y env√≠a un email de confirmaci√≥n `‚úÖ RECUPERADO`.

---

## 6. üìä Conclusi√≥n

La implementaci√≥n de esta arquitectura distribuida con monitoreo activo garantiza que **Quantitative Risk Management** sea un sistema:
1.  **Robusto**: Resistente a fallos individuales.
2.  **Confiable**: Con detecci√≥n de problemas en tiempo real.
3.  **Profesional**: Adecuado para entornos empresariales con SLAs definidos.
